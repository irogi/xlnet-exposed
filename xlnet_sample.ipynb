{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "fvFvBLJV0Dkv",
    "outputId": "140119e5-4cee-4604-c0d2-be279c18b125"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# XLNet specifics\n",
    "import torch\n",
    "from transformers import XLNetModel, XLNetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset info\n",
    "Using dataset SST2 (XLNet was able to get 94.4% accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv',\n",
    "                 delimiter='\\t',\n",
    "                 names=['sentence','label'])\n",
    "\n",
    "df_test = pd.read_csv('https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/test.tsv',\n",
    "                 delimiter='\\t',\n",
    "                 names=['sentence','label'])\n",
    "\n",
    "split_point = len(df_train)\n",
    "df = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyoj29J24hPX"
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv',\n",
    "#                  delimiter='\\t',\n",
    "#                  names=['sentence','label'])\n",
    "# df = df[:2000] # take only 2000 sentences for speed rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "jGvcfcCP5xpZ",
    "outputId": "4c4a8afc-1035-4b21-ba9a-c4bb6cfc6347"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3610\n",
       "0    3310\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    912\n",
       "1    909\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_MO08_KiAOb"
   },
   "source": [
    "# Training Model\n",
    "\n",
    "Training an XLnet from scratch is a very complicated task. Because of this, we instead use a pretrained, light version of the model (xlnet-base-cased: 110M params), and grabbed embeddings to put into a classification layer. This is different from the results from XLNet.\n",
    "\n",
    "Typically, the way to use XLNet would be to take the pretrained version (xlnet-large-cased), and then fine-tune to the data set. This would yield the high results, as found at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "q1InADgf5xm2",
    "outputId": "dbc52856-4d52-42f8-8a74-a89944280a02"
   },
   "outputs": [],
   "source": [
    " # See here for all pretrained https://huggingface.co/transformers/pretrained_models.html?highlight=pretrained\n",
    "pretrained_label = 'xlnet-base-cased'\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(pretrained_label)\n",
    "model = XLNetModel.from_pretrained(pretrained_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dg82ndBA5xlN"
   },
   "outputs": [],
   "source": [
    "tokenized = df['sentence'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These added special tokens include a classifier token, '<clf>' which we are mainly interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer string form: <cls>, id: 3\n"
     ]
    }
   ],
   "source": [
    "print(f'Tokenizer string form: {tokenizer.cls_token}, id: {tokenizer.cls_token_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URn-DWJt5xhP"
   },
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "clf_positions = np.zeros(len(tokenized), dtype=np.int64)\n",
    "for posn, i in enumerate(tokenized.values):\n",
    "    clf_positions[posn] = len(i) - 1\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mdjg306wjjmL"
   },
   "source": [
    "A sample padded sentence looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   24, 16003,    17,    19,  5787,    21,  1381, 21469,    17,\n",
       "          88,  7693, 15930,    56,    20,  4111,    21,    18, 11740,\n",
       "          21,  4974,    23,  6941,  2701,     4,     3,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the cls token is at the end of the sentence, before the padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDZBsYSDjzDV"
   },
   "source": [
    "### Masking\n",
    "A slight nuance, we need to pass in a attention masking map, which allows XLNet to identify where the sentence is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4K_iGRNa_Ozc",
    "outputId": "d03b0a9b-1f6e-4e32-831e-b04f5389e57c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8741, 86)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "We need to pass the tokenized sentences into XLNet now, and get the embeddings to pass into\n",
    "another classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39UVjAV56PJz"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(inputs, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to select the correct term, we select the classifier token from the mapping.\n",
    "We know that the final hidden state maps the classifier tokens to their respective position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "final_hidden_state = last_hidden_states[0]\n",
    "X = final_hidden_state[np.arange(len(final_hidden_state)), clf_positions]\n",
    "\n",
    "# Labels\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iaoEvM2evRx1"
   },
   "source": [
    "## Classification\n",
    "\n",
    "With all of the encoded values, now we can pass into any classifier we'd like to. For simplicity,\n",
    "we chose to take the default values of the sklearn MLPClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddAqbkoU6PP9"
   },
   "outputs": [],
   "source": [
    "num_test_pts = len(df) - split_point\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=num_test_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "gG-EVWx4CzBc",
    "outputId": "9252ceff-a7d0-4359-fef9-2f72be89c7d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iCoyxRJ7ECTA",
    "outputId": "cfd86dea-5d16-476c-ab9b-47cbee3a014f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7874794069192751"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot better than random guessing! It doesn't reach XLNets full accuracy in the paper (94.4%), but this model has:  \n",
    "- Simple, unoptimized classification layer\n",
    "- Fewer, untuned weights in XLNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning XLNet\n",
    "\n",
    "Although the heursitic test above is fast and easy to implement, it doesn't obtain the results\n",
    "found in the XLNet paper (94.4% accuracy).\n",
    "\n",
    "By running fine tuning on the SST-2 dataset, we were able to up the accuracy to 94.0%. This was accomplished\n",
    "by setting up the transformers repo, and running the [examples](https://github.com/huggingface/transformers/blob/master/examples/README.md):\n",
    "\n",
    "```bash\n",
    "export GLUE_DIR=/path/to/data\n",
    "export TASK_NAME=SST-2\n",
    "\n",
    "python run_glue.py \\\n",
    "  --model_type xlnet \\\n",
    "  --model_name_or_path xlnet-base-cased \\\n",
    "  --task_name $TASK_NAME \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --data_dir $GLUE_DIR/$TASK_NAME \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_gpu_train_batch_size 16 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 3.0 \\\n",
    "  --output_dir $GLUE_DIR/tmp/$TASK_NAME/\n",
    "```\n",
    "\n",
    "yields:\n",
    "```bash\n",
    "$ cat eval_results.txt\n",
    "acc = 0.9403669724770642\n",
    "```\n",
    "\n",
    "This task took significantly longer to do, but was able to fine tune to a particular data set incredibly well."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "A Visual Notebook to Using BERT for the First Time.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
