{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Hello World NLP Project\n",
    "\n",
    "#### Members Names: Oscar Bobadilla, Igor Ilic\n",
    "\n",
    "#### Members Emails: {oscar.bobadilla, iilic} @ ryerson.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "#### Problem Description:\n",
    "\n",
    "- A need for better Language modeling:\n",
    " - NLP Model forerunners: ELMo and ULMFiT(LSTM-Based)\n",
    " - BERT (Bidirectional Transformer architecture)  [1](http://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/)\n",
    " - XLNet (Transformer-XL architecture)  [1](https://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/)\n",
    "\n",
    "    Previous models, ELMo and ULMFiT were LSTM-Based an improvement from the word embeddings models. BERT, utilizes transformer architecture which allowed it to produce State-of-the-art results. XLNet uses Transfomer-XL, introduces the notion of relative positional embeddings. In addition, the Transformer XL architecture solves the problem of the transformer, which takes in fixed-length sequences as input. \n",
    "    \n",
    "    XLNet has produced State-of-the art results in the following tasks:\n",
    "       \n",
    "    -Text classification\n",
    "    -Question answering\n",
    "    -Natural language inference\n",
    "    -Duplicate sentence (question) detection\n",
    "    -Document ranking\n",
    "    -Coreference resolution\n",
    "    \n",
    "    - Reading Comprehension: \n",
    "     - Text Classification: \n",
    "     - Question Answering: Given information, can you answer a question? (Measured using SQuaD v1.1 / v2.0)\n",
    "     - Sentiment Analysis: Can you figure out if a statment is good / bad? (SST2)\n",
    "     - *GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task*\n",
    "\n",
    "\n",
    "***\n",
    "#### Context of the Problem:\n",
    "\n",
    "- Existing models and BERT\n",
    "\n",
    "    Prevous Language models, ELMo and ULMFiT, generally trained their models from \"left to right\". They were given a sequence of words, then have to predict the next word. The key traits of BERT, instead of predicting the next word after a sequence of words, BERT randomly masks words in a sentence and predicts them. This method is effective because it forces the model to learn how to use information from the entire sentence in deducing what words are missing.\n",
    "    \n",
    "\n",
    "- XLNet-TransformerXL\n",
    "\n",
    "    The autoregressive pretraining method enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT due to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. \n",
    "\n",
    "***\n",
    "#### Limitation About other Approaches:\n",
    "**BERT**\n",
    "1. The [MASK] token used in training does not appear during fine-tuning BERT is trained to predict tokens replaced with the special [MASK] token. The problem is that the [MASK] token - which is at the center of training \n",
    "\n",
    "    BERT never appears when fine-tuning BERT on downstream tasks. This can cause a whole host of issues such as:   \n",
    " - What does BERT do for tokens that are not replaced with [MASK]?\n",
    " - In most cases, BERT can simply copy non-masked tokens to the output. So would it really learn to produce meaningful representations for non-masked tokens?\n",
    " - Of course, BERT still needs to accumulate information from all words in a sequence to denoise [MASK] tokens. But what happens if there are no [MASK] tokens in the input sentence?\n",
    "\n",
    "   There are no clear answers to the above problems, but it's clear that the [MASK] token is a source of train-test skew that can cause problems during fine-tuning\n",
    "   \n",
    "\n",
    "2. BERT generates predictions independently. Another problem stems from the fact that BERT predicts masked tokens in parallel, meaning that during training, it does not learn to handle dependencies between predicting simultaneously masked tokens. In other words, it does not learn dependencies between its own predictions. \n",
    "\n",
    "   Since BERT is not actually used to unmask tokens, this is not directly a problem. The reason this can be a problem is that this reduces the number of dependencies BERT learns at once, making the learning signal weaker than it could be.\n",
    "    \n",
    "<img src=\"conceptual_difference.png\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />\n",
    "\n",
    "\n",
    "  \n",
    "***\n",
    "#### Solution:\n",
    "\n",
    "HOW DOES XLNet do what it does\n",
    "\n",
    "**XLNet**\n",
    "- TransformerXL USES:\n",
    "\n",
    "    XLNet addresses the previously mentioned problems by introducing a variant of language modeling called \"permutation language modeling\". Permutation language models are trained to predict one token given preceding context like traditional language model, but instead of predicting the tokens in sequential order, it predicts tokens in some random order. \n",
    "    The conceptual difference between BERT and XLNet. Transparent words are masked out so the model cannot rely on them. XLNet learns to predict the words in an arbitrary order but in an autoregressive, sequential manner (not necessarily left-to-right). BERT predicts all masked words simultaneously.\n",
    "\n",
    "    Aside from using permutation language modeling, XLNet improves upon BERT by using the Transformer XL as its base architecture. The Transformer XL showed state-of-the-art performance in language modeling, so was a natural choice for XLNet.\n",
    "\n",
    "    XLNet uses the two key ideas from Transformer XL: relative positional embeddings and the recurrence mechanism. The hidden states from the previous segment are cached and frozen while conducting the permutation language modeling for the current segment. Since all the words from the previous segment are used as input, there is no need to know the permutation order of the previous segment.\n",
    "\n",
    "\n",
    "Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language\n",
    "inference, sentiment analysis, and document ranking.1\n",
    ".\n",
    "\n",
    "[1](https://towardsdatascience.com/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335)\n",
    "\n",
    "<img src=\"results.png\" alt=\"Results of tests\" title=\"Title text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "SUMMARY OF RESULTS FROM PAPER\n",
    "\n",
    "| Reference |Explanation |  Dataset/Input |Weakness\n",
    "| --- | --- | --- | --- |\n",
    "| Tom et al. [1] | They trained a BERT based transformer to predict answers from the passage of a question| SQUAD dataset for QA | Only 80% accuracy\n",
    "| George et al. [2] | They trained a attention based sequence to sequence model using LSTM to predict answers from the passage of a question| SQUAD V2 dataset for QA | High accuracy but poor on unkown answers\n",
    "|J.Devlin et al.[10] | Use 3 BERT variants(Original BERT, BERT w/whole word masking, BERT w/o next sentence prediction). Utilizing the same data and hyperparameters for comparison| BooksCorpus[40],English Wikipedia, Giga5, ClueWeb 2012-B and Common Crawl for pretraining| XLNet outperforms BERT by a sizable margin on all the considered datasets. Ref. 1 (below)\n",
    "|Y. Liu et al[21], Z.Lan et al[19]| Comparison to other pre-trained models: RoBERTa, BERT+DCMN | Full Data -BooksCorpus[40],English Wikipedia, Giga5, ClueWeb 2012-B and Common Crawl for pretraining| Ref. 2,Ref. 3 ,Ref. 4 and Ref. 5 . XLNet generally outperforms BERT and RoBERTa\n",
    "\n",
    "1. <img src=\"table_1.png\" alt=\"Results of tests\" title=\"Title text\" />\n",
    "***\n",
    "**Ref.2 Reading comprehension & document ranking**\n",
    "2. <img src=\"table_2.png\" alt=\"Results of tests\" title=\"Title text\" />\n",
    "***\n",
    "**Ref.3 Question answering**\n",
    "3. <img src=\"table_3.png\" alt=\"Results of tests\" title=\"Title text\" />\n",
    "***\n",
    "**Ref.4 Text classification**\n",
    "4. <img src=\"table_4.png\" alt=\"Results of tests\" title=\"Title text\" />\n",
    "***\n",
    "**Ref.5 Natural language understanding\n",
    "5. <img src=\"table_5.png\" alt=\"Results of tests\" title=\"Title text\" />\n",
    "\n",
    "\n",
    "The last row in this table should be about the method discussed in this paper (If you can't find the weakenss of this method then write about the future improvement, see the future work section of the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "Though there are many ways to use bidirectional transformers, the way\n",
    "we explored was classification with the SST2 database. This dataset is commonly used as a benchmark,\n",
    "so we determined it to be a good place to explore.\n",
    "\n",
    "SST-2 consists of many different strings, which are classified as positive (1) or negative (0).\n",
    "Samples have been included below.\n",
    "\n",
    "Training an XLnet from scratch is a very complicated task. Because of this, we instead use a pretrained, light version of the model (xlnet-base-cased: 110M params), and grabbed embeddings to put into a classification layer. This is different from the results from XLNet.\n",
    "\n",
    "Typically, the way to use XLNet would be to take the pretrained version (xlnet-large-cased), and then fine-tune to the data set. This would yield the high results in the paper. We briefly discuss this at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "fvFvBLJV0Dkv",
    "outputId": "140119e5-4cee-4604-c0d2-be279c18b125"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# XLNet specifics\n",
    "import torch\n",
    "from transformers import XLNetModel, XLNetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset info\n",
    "Using dataset SST2 (XLNet was able to get 94.4% accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv',\n",
    "                 delimiter='\\t',\n",
    "                 names=['sentence','label'])\n",
    "\n",
    "df_test = pd.read_csv('https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/test.tsv',\n",
    "                 delimiter='\\t',\n",
    "                 names=['sentence','label'])\n",
    "\n",
    "split_point = len(df_train)\n",
    "df = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 1: a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films\n",
      "Label 0: apparently reassembled from the cutting room floor of any given daytime soap\n",
      "Label 0: they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science fiction elements of bug eyed monsters and futuristic women in skimpy clothes\n",
      "Label 1: this is a visually stunning rumination on love , memory , history and the war between art and commerce\n",
      "Label 1: jonathan parker 's bartleby should have been the be all end all of the modern office anomie films\n"
     ]
    }
   ],
   "source": [
    "for s, l in df.head().values:\n",
    "    print(f'Label {l}: {s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "jGvcfcCP5xpZ",
    "outputId": "4c4a8afc-1035-4b21-ba9a-c4bb6cfc6347"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3610\n",
       "0    3310\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    912\n",
       "1    909\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_MO08_KiAOb"
   },
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "q1InADgf5xm2",
    "outputId": "dbc52856-4d52-42f8-8a74-a89944280a02"
   },
   "outputs": [],
   "source": [
    " # See here for all pretrained https://huggingface.co/transformers/pretrained_models.html?highlight=pretrained\n",
    "pretrained_label = 'xlnet-base-cased'\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(pretrained_label)\n",
    "model = XLNetModel.from_pretrained(pretrained_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dg82ndBA5xlN"
   },
   "outputs": [],
   "source": [
    "tokenized = df['sentence'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These added special tokens include a classifier token, \"\\< cls\\>\" which we are mainly interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer string form: <cls>, id: 3\n"
     ]
    }
   ],
   "source": [
    "print(f'Tokenizer string form: {tokenizer.cls_token}, id: {tokenizer.cls_token_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URn-DWJt5xhP"
   },
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "clf_positions = np.zeros(len(tokenized), dtype=np.int64)\n",
    "for posn, i in enumerate(tokenized.values):\n",
    "    clf_positions[posn] = len(i) - 1\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mdjg306wjjmL"
   },
   "source": [
    "A sample padded sentence looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   24, 16003,    17,    19,  5787,    21,  1381, 21469,    17,\n",
       "          88,  7693, 15930,    56,    20,  4111,    21,    18, 11740,\n",
       "          21,  4974,    23,  6941,  2701,     4,     3,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the cls token is at the end of the sentence, before the padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDZBsYSDjzDV"
   },
   "source": [
    "### Masking\n",
    "A slight nuance, we need to pass in a attention masking map, which allows XLNet to identify where the sentence is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4K_iGRNa_Ozc",
    "outputId": "d03b0a9b-1f6e-4e32-831e-b04f5389e57c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8741, 86)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "We need to pass the tokenized sentences into XLNet now, and get the embeddings to pass into\n",
    "another classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39UVjAV56PJz"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(inputs, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to select the correct term, we select the classifier token from the mapping.\n",
    "We know that the final hidden state maps the classifier tokens to their respective position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "final_hidden_state = last_hidden_states[0]\n",
    "X = final_hidden_state[np.arange(len(final_hidden_state)), clf_positions]\n",
    "\n",
    "# Labels\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iaoEvM2evRx1"
   },
   "source": [
    "## Classification\n",
    "\n",
    "With all of the encoded values, now we can pass into any classifier we'd like to. For simplicity,\n",
    "we chose to take the default values of the sklearn MLPClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddAqbkoU6PP9"
   },
   "outputs": [],
   "source": [
    "num_test_pts = len(df) - split_point\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=num_test_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "gG-EVWx4CzBc",
    "outputId": "9252ceff-a7d0-4359-fef9-2f72be89c7d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iCoyxRJ7ECTA",
    "outputId": "cfd86dea-5d16-476c-ab9b-47cbee3a014f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7874794069192751"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot better than random guessing! It doesn't reach XLNets full accuracy in the paper (94.4%), but this model has:  \n",
    "- Simple, unoptimized classification layer\n",
    "- Fewer, untuned weights in XLNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning XLNet\n",
    "\n",
    "Although the heursitic test above is fast and easy to implement, it doesn't obtain the results\n",
    "found in the XLNet paper (94.4% accuracy).\n",
    "\n",
    "By running fine tuning on the SST-2 dataset, we were able to up the accuracy to 94.0%. This was accomplished\n",
    "by setting up the transformers repo, and running the [examples](https://github.com/huggingface/transformers/blob/master/examples/README.md):\n",
    "\n",
    "```bash\n",
    "export GLUE_DIR=/path/to/data\n",
    "export TASK_NAME=SST-2\n",
    "\n",
    "python run_glue.py \\\n",
    "  --model_type xlnet \\\n",
    "  --model_name_or_path xlnet-base-cased \\\n",
    "  --task_name $TASK_NAME \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --data_dir $GLUE_DIR/$TASK_NAME \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_gpu_train_batch_size 16 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 3.0 \\\n",
    "  --output_dir $GLUE_DIR/tmp/$TASK_NAME/\n",
    "```\n",
    "\n",
    "yields:\n",
    "```bash\n",
    "$ cat eval_results.txt\n",
    "acc = 0.9403669724770642\n",
    "```\n",
    "\n",
    "This task took significantly longer to do, but was able to fine tune to a particular data set incredibly well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Direction\n",
    "\n",
    "Write few sentences about the results and their limitations, how they can be extended in future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]:  Authors names, title of the paper, Conference Name,Year, page number (iff available)\n",
    "\n",
    "[2]:  Author names, title of the paper, Journal Name,Journal Vol, Issue Num, Year, page number (iff available)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
