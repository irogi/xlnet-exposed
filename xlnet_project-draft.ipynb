{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Hello World NLP Project\n",
    "\n",
    "#### Members Names: Oscar Bobadilla, Igor Ilic\n",
    "\n",
    "#### Members Emails: {oscar.bobadilla, iilic} @ ryerson.ca"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "#### Problem Description:\n",
    "\n",
    "- A need for better Language modeling:\n",
    " - NLP Model forerunners: ELMo and ULMFiT(LSTM-Based)\n",
    " - BERT (Bidirectional Transformer architecture)  [1](http://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/)\n",
    " - XLNet (Transformer-XL architecture)  [1](https://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/)\n",
    "\n",
    "    Previous models, ELMo and ULMFiT were LSTM-Based an improvement from the word embeddings models. BERT, utilizes transformer architecture which allowed it to produce State-of-the-art results. XLNet uses Transfomer-XL, introduces the notion of relative positional embeddings. In addition, the Transformer XL architecture solves the problem of the transformer, which takes in fixed-length sequences as input. \n",
    "    \n",
    "    XLNet has produced State-of-the art results in the following tasks:\n",
    "     - Reading Comprehension: \n",
    "     - Text Classification: \n",
    "     - Question Answering: Given information, can you answer a question? (Measured using SQuaD v1.1 / v2.0)\n",
    "     - Sentiment Analysis: Can you figure out if a statment is good / bad? (SST2)\n",
    "     - *GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task*\n",
    "\n",
    "\n",
    "***\n",
    "#### Context of the Problem:\n",
    "\n",
    "- Existing models and BERT\n",
    "\n",
    "    Prevous Language models, ELMo and ULMFiT, generally trained their models from \"left to right\". They were given a sequence of words, then have to predict the next word. The key traits of BERT, instead of predicting the next word after a sequence of words, BERT randomly masks words in a sentence and predicts them. This method is effective because it forces the model to learn how to use information from the entire sentence in deducing what words are missing.\n",
    "    \n",
    "\n",
    "- XLNet-TransformerXL\n",
    "\n",
    "    The autoregressive pretraining method enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and overcomes the limitations of BERT due to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. \n",
    "\n",
    "***\n",
    "#### Limitation About other Approaches:\n",
    "**BERT**\n",
    "1. The [MASK] token used in training does not appear during fine-tuning BERT is trained to predict tokens replaced with the special [MASK] token. The problem is that the [MASK] token - which is at the center of training \n",
    "\n",
    "    BERT never appears when fine-tuning BERT on downstream tasks. This can cause a whole host of issues such as:   \n",
    " - What does BERT do for tokens that are not replaced with [MASK]?\n",
    " - In most cases, BERT can simply copy non-masked tokens to the output. So would it really learn to produce meaningful representations for non-masked tokens?\n",
    " - Of course, BERT still needs to accumulate information from all words in a sequence to denoise [MASK] tokens. But what happens if there are no [MASK] tokens in the input sentence?\n",
    "\n",
    "   There are no clear answers to the above problems, but it's clear that the [MASK] token is a source of train-test skew that can cause problems during fine-tuning\n",
    "   \n",
    "\n",
    "2. BERT generates predictions independently. Another problem stems from the fact that BERT predicts masked tokens in parallel, meaning that during training, it does not learn to handle dependencies between predicting simultaneously masked tokens. In other words, it does not learn dependencies between its own predictions. \n",
    "\n",
    "   Since BERT is not actually used to unmask tokens, this is not directly a problem. The reason this can be a problem is that this reduces the number of dependencies BERT learns at once, making the learning signal weaker than it could be.\n",
    "    \n",
    "<img src=\"conceptual_difference.png\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />\n",
    "\n",
    "\n",
    "  \n",
    "***\n",
    "#### Solution:\n",
    "\n",
    "HOW DOES XLNet do what it does\n",
    "\n",
    "**XLNet**\n",
    "- TransformerXL USES:\n",
    "\n",
    "    XLNet addresses the previously mentioned problems by introducing a variant of language modeling called \"permutation language modeling\". Permutation language models are trained to predict one token given preceding context like traditional language model, but instead of predicting the tokens in sequential order, it predicts tokens in some random order. \n",
    "    The conceptual difference between BERT and XLNet. Transparent words are masked out so the model cannot rely on them. XLNet learns to predict the words in an arbitrary order but in an autoregressive, sequential manner (not necessarily left-to-right). BERT predicts all masked words simultaneously.\n",
    "\n",
    "    Aside from using permutation language modeling, XLNet improves upon BERT by using the Transformer XL as its base architecture. The Transformer XL showed state-of-the-art performance in language modeling, so was a natural choice for XLNet.\n",
    "\n",
    "    XLNet uses the two key ideas from Transformer XL: relative positional embeddings and the recurrence mechanism. The hidden states from the previous segment are cached and frozen while conducting the permutation language modeling for the current segment. Since all the words from the previous segment are used as input, there is no need to know the permutation order of the previous segment.\n",
    "\n",
    "\n",
    "Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language\n",
    "inference, sentiment analysis, and document ranking.1\n",
    ".\n",
    "\n",
    "[1](https://towardsdatascience.com/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335)\n",
    "\n",
    "<img src=\"results.png\" alt=\"Results of tests\" title=\"Title text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "SUMMARY OF RESULTS FROM PAPER\n",
    "\n",
    "| Reference |Explanation |  Dataset/Input |Weakness\n",
    "| --- | --- | --- | --- |\n",
    "| Tom et al. [1] | They trained a BERT based transformer to predict answers from the passage of a question| SQUAD dataset for QA | Only 80% accuracy\n",
    "| George et al. [2] | They trained a attention based sequence to sequence model using LSTM to predict answers from the passage of a question| SQUAD V2 dataset for QA | High accuracy but poor on unkown answers\n",
    "\n",
    "\n",
    "The last row in this table should be about the method discussed in this paper (If you can't find the weakenss of this method then write about the future improvement, see the future work section of the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "- Explain text classification/sentiment analysis problem\n",
    "- Describe SST2 database\n",
    "- Talk about XLNet + Classifier\n",
    "\n",
    "Provide details of the method that you are implementing in the next section with figure(s). A fifg Your methodology will be just one method discussed in one of the paper of your choice; it can be a merger or a simplified version of the papers. To avoid any confusion, do not present multiple methods, just one unified method as you will implement in the next section.\n",
    "\n",
    "For figures you can use this tag:\n",
    "\n",
    "![Alternate text ](Figure.png \"Title of the figure, location is simply the directory of the notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "In this section, you will provide the code and its explanation. You may have to create more cells after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Direction\n",
    "\n",
    "Write few sentences about the results and their limitations, how they can be extended in future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "[1]:  Authors names, title of the paper, Conference Name,Year, page number (iff available)\n",
    "\n",
    "[2]:  Author names, title of the paper, Journal Name,Journal Vol, Issue Num, Year, page number (iff available)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
